{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = ''\n",
    "db_name = 'random_train.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(loc+db_file)\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_connection(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if con is not None:\n",
    "    tag_data = pd.read_sql('SELECT Tags FROM data',con)\n",
    "else :\n",
    "    print('Conn error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tags\n",
      "0             iphone objective-c ios uiview uibutton\n",
      "1                    svg internet-explorer-9 raphael\n",
      "2  validation spring-mvc internationalization cus...\n",
      "3                             windows java copy text\n",
      "4                                  javascript jquery\n",
      "('Shape', (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "print(tag_data.head())\n",
    "print('Shape',tag_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "if con is not None:\n",
    "    t_data = pd.read_sql('SELECT Title FROM data',con)\n",
    "else :\n",
    "    print('Conn error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title\n",
      "0       One tap triggering events on multiple views?\n",
      "1  IE9 text positioning bug when zoomed in with R...\n",
      "2  Spring MVC custom errors and internationalization\n",
      "3                How to copy text from Java program?\n",
      "4  How to scroll to a part of the page using jQuery?\n",
      "('Shape', (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "print(t_data.head())\n",
    "print('Shape',t_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess title Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "# https://stackoverflow.com/questions/35345761/python-re-split-vs-nltk-word-tokenize-and-sent-tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "t_data.head()\n",
    "t_data.Title = t_data.Title.apply(lambda x : x.encode('utf-8'))\n",
    "t_data.Title = t_data.Title.apply(lambda x : str.lower(x))\n",
    "t_data.Title = t_data.Title.apply(lambda x : re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#title_data = t_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(sen):\n",
    "    return ' '.join(w for w in word_tokenize(sen) if w not in stop_words)\n",
    "\n",
    "t_data.Title = t_data.Title.apply(lambda x : fn(x))#' '.join for w in word_tokenize(x) if w not in stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         one tap triggering events multiple views\n",
       "1       ie text positioning bug zoomed raphael svg\n",
       "2    spring mvc custom errors internationalization\n",
       "3                           copy text java program\n",
       "4                    scroll part page using jquery\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data.Title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'doing',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'has',\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'have',\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\",\n",
       " u'y',\n",
       " u'you',\n",
       " u\"you'd\",\n",
       " u\"you'll\",\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'wive'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer.stem('wives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = t_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')  #Binary BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lab_y = vectorizer.fit_transform(tag_data.Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of data points :', 10000)\n",
      "('Number of unique tags :', 6205)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points :\", multi_lab_y.shape[0])\n",
    "print(\"Number of unique tags :\", multi_lab_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose first n tags (desc order of count)\n",
    "\n",
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn = multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.2 Split the data into test and train (80:20) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8000)\n"
     ]
    }
   ],
   "source": [
    "tot_size = title_data.shape[0]\n",
    "train_size = int(tot_size * 0.8)\n",
    "print(tot_size,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=title_data.head(train_size)\n",
    "x_test=title_data.tail(tot_size - train_size)\n",
    "\n",
    "y_train = multi_lab_y[0:train_size,:]\n",
    "y_test = multi_lab_y[train_size:tot_size,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @ Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time taken to run this cell :', datetime.timedelta(0, 0, 523872))\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train.Title)\n",
    "x_test_multilabel = vectorizer.transform(x_test.Title)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dimensions of train data X:', (8000, 69639), 'Y :', (8000, 6205))\n",
      "('Dimensions of test data X:', (2000, 69639), 'Y:', (2000, 6205))\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDC with OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy :', 0.049500000000000002)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
    "#print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "#print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "#print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
    "#print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic with OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time taken to run this cell :', datetime.timedelta(0, 89, 9685))\n",
      "('accuracy :', 0.043499999999999997)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier2 = OneVsRestClassifier(LogisticRegression(penalty='l1'), n_jobs=-1)\n",
    "classifier2.fit(x_train_multilabel, y_train)\n",
    "predictions2 = classifier2.predict(x_test_multilabel)\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA  -- TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n"
     ]
    }
   ],
   "source": [
    "y_train3 =y_train.todense()\n",
    "y_test3 = y_test.todense()\n",
    "\n",
    "print type(y_train)\n",
    "print type(x_train_multilabel.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#classifier3 = OneVsRestClassifier(LDA())\n",
    "#classifier3.fit(x_train_multilabel.todense(), y_train)\n",
    "#predictions3 = classifier3.predict(x_test_multilabel)\n",
    "\n",
    "#print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "#print(\"accuracy :\",metrics.accuracy_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time taken to run this cell :', datetime.timedelta(0, 69, 99169))\n",
      "('accuracy :', 0.035999999999999997)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier4 = OneVsRestClassifier(LinearSVC())\n",
    "classifier4.fit(x_train_multilabel, y_train)\n",
    "predictions4 = classifier4.predict(x_test_multilabel)\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
